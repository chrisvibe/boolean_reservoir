logging:
  out_path: 'out/temporal/density/grid_search/design_choices_prep/all'
  save_keys: null 
  grid_search:
    n_samples: 1
model:
  input_layer:
    w_bi: identity
    w_ir: identity 
    pertubation: override
    encoding: [base2, 'binary_embedding']
    features: [1, 2] # affects how redundancy is applied
    redundancy: [1, 2, 3] # changes the task complexity
    resolution: 10
    chunk_size: 10 # set to resolution
    ticks: ['1', '2']
    interleaving: [0, 1]
  output_layer:
    activation: sigmoid
    n_outputs: 1 # no effect on kq and gr
  reservoir_layer:
    init: random # not much effect 
    k_avg: [1.0, 2.0, 3.0, 4.0, 5.0]
    k_min: 0
    k_max: ['k_avg + 5', 10] # interesting number of attractors
    mode: [homogeneous, heterogeneous]
    n_nodes: [512, 1024] # make sure 2**D.bits-1 >= n_nodes to have enough states
    reset: true
    self_loops: [0, .1, .3, .5, 1] # interesting number of attractors
  training:
    accuracy_threshold: 0.5
    batch_size: 1024 # note: = train samples
    criterion: BCE
    epochs: 40
    evaluation: dev
dataset:
  train:
    task: density
    bits: 10
    window: [3, 5]
    delay: [0, 3, 5]
    split:
      train: 0.5
      dev: 0.5
      test: 0
    samples: 2048 # 2**10=1024 input combinations, twice this ensures a copy for train/dev
    sampling_mode: 'exhaustive'
    seed: 0
  kqgr:
    bits: 10
    split:
      train: 1
      dev: 0
      test: 0
    evaluation: ['first', 'last', 'random']
    tau: [3, 5]
    seed: 0