# attempt to reproduce figure 3:
logging:
  out_path: out/grid_search/temporal/parity/initial_sweep
  grid_search:
    seed: 0
    n_samples: 10
model:
  input_layer:
    distribution: identity
    connection: [out-0:b:1/(a+b), out-1:b:1/(a+b)] # "I additional input nodes that distribute the input signals to the nodes in the network. The source node of Ki links for each node i is randomly picked from N + I nodes with uniform probability" (their N is my R)
    encoding: base2
    features: 10
    n_nodes: 1
    pertubation: override
    redundancy: 1
    bits_per_feature: 1
    seed: 0
  output_layer:
    activation: sigmoid
    n_outputs: 1
    seed: 0
  reservoir_layer:
    init: random
    k_min: 0
    k_avg: [1, 2, 3, 4, 5, 6, 7, 8] # 2.25 (optimal for heterogenous) 2 (optimal for homogenous)
    k_max: 20 # no k_max in their paper (needed given scalability of current implementation)
    mode: [heterogenous, homogenous]
    n_nodes: 500 # N=500 for "task solving" and N=25 for kQ and GR
    p: 0.5
    reset: True # no explicit mention, assume reset: "remember perturbations by an external input over time"
    self_loops: null # no control over self-loops (random)
    seed: 0
  training: # optimizer is not specified but is gradient descent-based (i use adam optimizer on a sigmoid activated readout with binary-cross entropy cost function)
    accuracy_threshold: 0.5
    batch_size: 100
    criterion: BCE
    epochs: 40 # epochs is different
    evaluation: dev
    learning_rate: 0.001 # learning rate is different
    seed: 0
dataset:
  task: parity
  bit_stream_length: 10
  window_size: 3
  tao: [1, 3, 5, 7]
  split:
    train: 0.4
    dev: 0.3
    test: 0.3
  samples: 6000
  seed: 0